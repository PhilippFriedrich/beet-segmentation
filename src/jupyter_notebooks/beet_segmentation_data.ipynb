{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f0365be-7486-494b-881b-8824bc3801a6",
   "metadata": {},
   "source": [
    "# Beet segmentation (optional) data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8cb0c1-9179-4941-bf8c-090a5f2ecb26",
   "metadata": {},
   "source": [
    "Date: 18.02.2024  \n",
    "Authors: Gustav Schimmer & Philipp Friedrich"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845e7430-5bee-4568-b8fa-b4c5b420b365",
   "metadata": {},
   "source": [
    "**This notebook is purposed for data preparation before training a YOLOv6 algorithm in detecting sugar beet plants on images.**  \n",
    "  \n",
    "  \n",
    "Major steps are:\n",
    "- Downsampling and Resizing of images\n",
    "- Create custom dataset with labeled data\n",
    "\n",
    "Before we train our model, we need to prepare a proper dataset containing images for testing and validation in the right resolution and size. This notebook is a suggestion on how to generate such a dataset. In case you dont want to generate and use own training data, we provided a ready to use example dataset, which can be found in the YOLOv6 section of this project (custom_dataset). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee60dc8-38b6-4767-b2d7-3b6401dc9efc",
   "metadata": {},
   "source": [
    "## Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7824674a-9d8c-4592-825b-0aa0c6140d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d42d93-75f6-4a14-8cdb-c152c31cf807",
   "metadata": {},
   "source": [
    "## Data preparation: Downsampling & Resizing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5248715-252e-47e8-80f7-1e2ad00942aa",
   "metadata": {},
   "source": [
    "Before creation of a custom dataset, data needs to be resampled to a lower resolution to minimize needed computation power."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528ef7a1-5d32-49a6-ba26-b5458421a57d",
   "metadata": {},
   "source": [
    "#### Define data paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c250c711-0018-4d95-afb5-62015d6cc900",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input data path\n",
    "input_folder = 'path/to/your/images..'\n",
    "\n",
    "# Output data path\n",
    "output_folder = 'path/to/store/your/images..'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdfb1d39-5555-47cb-86d2-54b0a415d15b",
   "metadata": {},
   "source": [
    "#### Write function to resample images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e342dad5-d8f7-4df3-82a3-e64bfc30113d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write function to resample images to taret width and height and crop it to squares\n",
    "def crop_and_resize_image(input_path, output_folder, square_size, output_size, target_width, target_height):\n",
    "    image = cv2.imread(input_path)\n",
    "    if image is not None:\n",
    "        \n",
    "        # Resize the image to the target width and height\n",
    "        image = cv2.resize(image, (target_width, target_height))\n",
    "        \n",
    "        # Crop the image into squares\n",
    "        for y in range(0, target_height - square_size + 1, square_size):\n",
    "            for x in range(0, target_width - square_size + 1, square_size):\n",
    "                \n",
    "                # Extract a square region from the image\n",
    "                cut_image = image[y:y + square_size, x:x + square_size]\n",
    "\n",
    "                # Ensure that the color channels are maintained correctly\n",
    "                cut_image_rgb = cv2.cvtColor(cut_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "                # Create a new PIL Image with the specified size and paste the cut image onto it\n",
    "                new_size = (output_size, output_size)\n",
    "                new_image = Image.new('RGB', new_size, 'black')\n",
    "                new_image.paste(Image.fromarray(cut_image_rgb), (0, 0))\n",
    "                \n",
    "                # Save the new image\n",
    "                output_path = os.path.join(output_folder, f\"{os.path.splitext(os.path.basename(input_path))[0]}_{y // square_size * (target_width // square_size) + x // square_size}.jpg\")\n",
    "                new_image.save(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c589e3f-6133-4cf5-8de9-545b6fbe6af8",
   "metadata": {},
   "source": [
    "#### Image resampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea9524b-9f02-4216-b93b-2e7caf20810e",
   "metadata": {},
   "source": [
    "As we tried different image resolutions, for our images a resolution of 2000x1500 yields a good compromise between results and computation power. This relates to a Ground Sampling Distance (GSD) of 0.1 centimeters. To make the images usable for YOLOv6 algorithm we additionally need to resize them to a squared size, for wich we use 512x512 pixels as it has to be a multiple of 32. However if we would cut the images of size 2000x1500 directly to 512x512 squares we would loose a lot of pixels due to incomplete division of 2000/1500 by 512. For this reason we devide the images into 500x500 squares and add pixels at the edges afterwards. The images in the input folder need to have the same size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3129e1f3-0c4b-43d2-8799-05afa955fd04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image resampling done.\n"
     ]
    }
   ],
   "source": [
    "# Define target image width and height\n",
    "target_width, target_height = 2000, 1500\n",
    "cut_size = 500\n",
    "final_size = 512\n",
    "\n",
    "# Create output folder if it doesn't exist\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Resize and Crop every image in the input folder\n",
    "for filename in os.listdir(input_folder):\n",
    "    if filename.endswith(\".jpg\"):\n",
    "        input_path = os.path.join(input_folder, filename)\n",
    "        crop_and_resize_image(input_path, output_folder, cut_size, final_size, target_width, target_height)\n",
    "\n",
    "print(\"Image resampling done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d28ff22-f872-4796-9030-092cabf96e84",
   "metadata": {},
   "source": [
    "## Create training lables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8e6822-7a42-4652-829e-ced5807b8c3f",
   "metadata": {},
   "source": [
    "To train the algorithm training data consisting of annotations are necessary. This often is cost and time intensive. Some of the open source tools available online are:\n",
    "\n",
    "- https://roboflow.com/annotate?ref=blog.roboflow.com\n",
    "\n",
    "- https://blog.roboflow.com/cvat/\n",
    "\n",
    "- https://blog.roboflow.com/labelimg/\n",
    "\n",
    "- https://www.makesense.ai/\n",
    "\n",
    "In our case we need to create lables for the single sugar beet plants on the images. If you want to create your own lables for your own images you can use one of the above mentioned tools. We recomend using Make Sense AI as it is open source and supports Yolo output file formats. \n",
    "\n",
    "\n",
    "One image should corresponds to one label file, and the label format example is presented as below.\n",
    "\n",
    "```json\n",
    "# class_id center_x center_y bbox_width bbox_height\n",
    "0 0.300926 0.617063 0.601852 0.765873\n",
    "1 0.575 0.319531 0.4 0.551562\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d36f9e-85a3-4827-b91c-4b8161d3652d",
   "metadata": {},
   "source": [
    "## Create custom dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17851fb6-64b1-46d9-8869-b715b79ff8e8",
   "metadata": {},
   "source": [
    "The generated images and lables should be devided into training and validation data (approximately 80:20). Organize your directory of the custom dataset as follows:\n",
    "\n",
    "```shell\n",
    "custom_dataset\n",
    "├── images\n",
    "│   ├── train\n",
    "│   │   ├── train0.jpg\n",
    "│   │   └── train1.jpg\n",
    "│   ├── val\n",
    "│   │   ├── val0.jpg\n",
    "│   │   └── val1.jpg\n",
    "│   └── test\n",
    "│       ├── test0.jpg\n",
    "│       └── test1.jpg\n",
    "└── labels\n",
    "    ├── train\n",
    "    │   ├── train0.txt\n",
    "    │   └── train1.txt\n",
    "    ├── val\n",
    "    │   ├── val0.txt\n",
    "    │   └── val1.txt\n",
    "    └── test\n",
    "        ├── test0.txt\n",
    "        └── test1.txt\n",
    "```\n",
    "\n",
    "Your custom datset is now ready to use. You can continue with model training in this [Jupyter Notebook](beet_segmentation_model.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f5e4b4-dc4f-49c0-8956-b660c06722cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
