{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3434362-473c-4195-b7ca-ccb68580fc1f",
   "metadata": {},
   "source": [
    "# Beet segmentation model training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4bc759-1424-47b0-ab6d-6f35b576affd",
   "metadata": {},
   "source": [
    "Date: 18.02.2024  \n",
    "Authors: Gustav Schimmer & Philipp Friedrich  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3939167e-0766-4fec-9746-b0a0965977eb",
   "metadata": {},
   "source": [
    "**This notebook is purposed for training a YOLOv6 algorithm in detecting sugar beet plants on images.**  \n",
    "  \n",
    "  \n",
    "Major steps are:\n",
    "- Initialize YOLOv6 algorithm\n",
    "- Train algorithm\n",
    "- Validation of the model\n",
    "- Inferencing YOLOv6 model on test data\n",
    "\n",
    "This notebook is designed to run from [Google Colab](https://colab.research.google.com/?hl=de). Before we start we need to make sure to have a proper dataset we can use. In case you don't have a dataset yet and want to use own custom data you can follow the steps in this [Jupyter Notebook](beet_segmentation_data.ipynb). Otherwise you can use the dataset we provided in the YOLOv6 section of this project (custom_dataset). Your dataset should be stored in your Google Drive for accessing it via Google Colab."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3e18e2-584b-4d9b-ae7a-0324fd24d733",
   "metadata": {},
   "source": [
    "## Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81d12aea-49b4-4274-bba9-ed22a1681563",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m drive\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from google.colab import drive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ccd421a-4933-4a97-b72d-2e54d7749f3a",
   "metadata": {},
   "source": [
    "Make sure your your notebook uses GPU to speed up model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc6958c-e4f7-4746-9299-181a6fdc4e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb6bb83-236c-4c0c-b0d2-6ae30cff731b",
   "metadata": {},
   "source": [
    "## Mount Drive for working in Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332753d4-77d3-47d6-86de-5f6cadf27c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "drive.mount('/content/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b26d85-aebb-4dc6-bd38-d3a9a6cac068",
   "metadata": {},
   "outputs": [],
   "source": [
    "HOME = os.getcwd()\n",
    "print(HOME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59b8b39-1d35-4117-98ae-3ffd6e0d32d6",
   "metadata": {},
   "source": [
    "## Initializing YOLOv6 algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274340fe-7752-4f52-9af2-19a74de8e1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download MT-YOLOv6 repository and install requirements\n",
    "!git clone https://github.com/meituan/YOLOv6\n",
    "%cd YOLOv6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230f93d0-3801-4942-8644-d0999be91d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b902fa27-fdc0-450b-abb7-f37e4d3c3a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1374d595-735a-4013-9256-c8140c6a2646",
   "metadata": {},
   "source": [
    "## Update dataset.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2980a8cb-bad9-4385-8fcb-d24141d23f54",
   "metadata": {},
   "source": [
    "Before we train our model we need to update the data/dataset.yml file with the following code, letting the model know where the data is located.\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "# Please insure that your custom_dataset are put in same parent dir with YOLOv6_DIR\n",
    "train: ./custom_dataset/images/train # train images\n",
    "val: ./custom_dataset/images/val # val images\n",
    "\n",
    "# whether it is coco dataset, only coco dataset should be set to True.\n",
    "is_coco: False\n",
    "# Classes\n",
    "nc: 1  # number of classes\n",
    "names: ['sugar beet']  # class names\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630369a0-e8a0-4da7-8fbb-da3774fc981d",
   "metadata": {},
   "source": [
    "## Copy dataset to YOLOv6 directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2336eae4-2a59-4067-979e-81eb3552800f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cp -r /content/drive/MyDrive/custom_dataset /content/YOLOv6/custom_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707eb87c-760c-41d2-8cea-a9c196c4a7cf",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945f358a-58c8-44cf-87d3-d5676a9e9e1c",
   "metadata": {},
   "source": [
    "Now we can train the model. Batch size and epochs can be adjusted. Try different settings to see which yield the best results.  \n",
    "\n",
    "Note the structure of the basic training command: we call a specific script (tools/train.py), set a batch size argument (--batch 256), set a specific configuration file (--conf configs/yolov6s.py), pass our data.yaml file, and set what type of CUDA device we're using (--device 0).\n",
    "\n",
    "The configuration file for training YOLOv6 comes with support for finetuning (e.g. calling configs/yolov6s_finetune.py vs starting from scratch (yolov6s.py). Finetuning will train faster though may not be as effective on unique datasets. It may make most sense if you're resuming training from a substantially similar domain, like adding more data to a previously completed training job.\n",
    "\n",
    "Moreover, YOLOv6 supports a high number of arguments by default. Here's what all the options available to us are, their default value, and what they mean:\n",
    "\n",
    "\n",
    "*  --data-path, default='./data/coco.yaml', type=str, help='path of dataset')\n",
    "*   --conf-file, default='./configs/yolov6s.py', type=str, help='experiments description file')\n",
    "*   --img-size, type=int, default=640, help='train, val image size (pixels)')\n",
    "*  --batch-size, default=32, type=int, help='total batch size for all GPUs')\n",
    "*   --epochs, default=400, type=int, help='number of total epochs to run')\n",
    "*   --workers, default=8, type=int, help='number of data loading workers (default: 8)')\n",
    "*   --device, default='0', type=str, help='cuda device, i.e. 0 or 0,1,2,3 or cpu')\n",
    "*   --eval-interval, type=int, default=20, help='evaluate at every interval epochs')\n",
    "*   --eval-final-only, action='store_true', help='only evaluate at the final epoch')\n",
    "*   --heavy-eval-range, default=50,help='evaluating every epoch for last such epochs (can be jointly used with --eval-interval)')\n",
    "*  --check-images, action='store_true', help='check images when initializing datasets')\n",
    "*   --check-labels, action='store_true', help='check label files when initializing datasets')\n",
    "*   --output-dir, default='./runs/train', type=str, help='path to save outputs')\n",
    "*   --name, default='exp', type=str, help='experiment name, saved to output_dir/name')\n",
    "*   --dist_url, type=str, default=\"default url: tcp://127.0.0.1:8888\")\n",
    "*   --gpu_count, type=int, default=0)\n",
    "*   --local_rank, type=int, default=-1, help='DDP parameter')\n",
    "*   --resume, type=str, default=None, help='resume the corresponding ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12907d2c-122b-4b11-a9e8-ee0595f06353",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /content/YOLOv6\n",
    "!python tools/train.py --batch 16 --conf configs/yolov6s_finetune.py --data data/dataset.yaml --fuse_ab --epochs 10 --img-size 512 --device 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e216dd3-4600-4221-892e-c61c8e24cc5a",
   "metadata": {},
   "source": [
    "## Evaulate YOLOv6 Model Performance\n",
    "\n",
    "YOLOv6 comes with a tools directory, one of which is for evaluation of model training. Evaluating the model's performance includes assessing the model's mean average precision (mAP), precision, and recall. For the uninitiated, a higher mAP score indicates our model is drawing the correct boxes in the right places.\n",
    "\n",
    "Similar to the training argument, we do have parameters we can adjust for how we evaluate the model (like specifying confidence thresholds or modifying where the results are saved).\n",
    "\n",
    "\n",
    "* --data, type=str, default='./data/coco.yaml', help='dataset.yaml path')\n",
    "* --weights, type=str, default='./weights/yolov6s.pt', help='model.pt path(s)')\n",
    "* --batch-size, type=int, default=32, help='batch size')\n",
    "* --img-size, type=int, default=640, help='inference size (pixels)')\n",
    "* --conf-thres, type=float, default=0.001, help='confidence threshold')\n",
    "* --iou-thres, type=float, default=0.65, help='NMS IoU threshold')\n",
    "* --task, default='val', help='val, or speed')\n",
    "* --device, default='0', help='cuda device, i.e. 0 or 0,1,2,3 or cpu')\n",
    "* --half, default=False, action='store_true', help='whether to use fp16 infer')\n",
    "* --save_dir, type=str, default='runs/val/', help='evaluation save dir')\n",
    "* --name, type=str, default='exp', help='save evaluation results to save_dir/name')\n",
    "\n",
    "The evaluation command we'll run is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8494f9-f634-4408-a9be-def9212a543d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /content/YOLOv6\n",
    "#make sure to enter the correct path to the best_ckpt.pt\n",
    "!python tools/eval.py --data data/dataset.yaml  --weights runs/train/exp/weights/best_ckpt.pt --img-size 512 --do_pr_metric True --plot_confusion_matrix --task val --device 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64a3440-759e-4fb0-a9d9-4c20b146b65b",
   "metadata": {},
   "source": [
    "# Inference Yolov6 on Test dataset\n",
    "\n",
    "Similar to YOLOv6 evaluation, there's an inference script that comes with the repository's tools. We had tested this our before. This inference script allows us to pass a batch of images in a given directory on which the model will run its predictions.\n",
    "\n",
    "In addition, this inference utility also comes with a number of arguments we can pass for things like displaying the labels on the predicted images (which is on by default), modifying the confidence level and NMS thresholds, and setting max detections.\n",
    "\n",
    "* --weights, type=str, default='weights/yolov6s.pt', help='model path(s) for inference.')\n",
    "* --source, type=str, default='data/images', help='the source path, e.g. image-file/dir.')\n",
    "* --yaml, type=str, default='data/coco.yaml', help='data yaml file.')\n",
    "* --img-size, type=int, default=640, help='the image-size(h,w) in inference size.')\n",
    "* --conf-thres, type=float, default=0.25, help='confidence threshold for inference.')\n",
    "* --iou-thres, type=float, default=0.45, help='NMS IoU threshold for inference.')\n",
    "* --max-det, type=int, default=1000, help='maximal inferences per image.')\n",
    "* --device, default='0', help='device to run our model i.e. 0 or 0,1,2,3 or cpu.')\n",
    "* --save-txt, action='store_true', help='save results to *.txt.')\n",
    "* --save-img, action='store_false', help='save visuallized inference results.')\n",
    "* --classes, nargs='+', type=int, help='filter by classes, e.g. --classes 0, or --classes 0 2 3.')\n",
    "* --agnostic-nms, action='store_true', help='class-agnostic NMS.')\n",
    "* --project, default='runs/inference', help='save inference results to project/name.')\n",
    "* --name, default='exp', help='save inference results to project/name.')\n",
    "* --hide-labels, default=False, action='store_true', help='hide labels.')\n",
    "* --hide-conf, default=False, action='store_true', help='hide confidences.')\n",
    "* --half, action='store_true', help='whether to use FP16 half-precision inference.')\n",
    "\n",
    "We'll run this command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1922a3-b724-4220-9fe8-5d872bd4244c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /content/YOLOv6\n",
    "!python tools/infer.py --yaml data/dataset.yaml --weights runs/train/exp/weights/best_ckpt.pt --source custom_dataset/images/val --img-size 512 512  --project 'runs/inference/sugarbeets' --device 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094dc615-6ff4-4559-a11d-8934ee7c53ac",
   "metadata": {},
   "source": [
    "## Converting Yolov6 to ONNX\n",
    "\n",
    "One additional utility YOLOv6 comes with is the ability to be converted to ONNX, a common model serialization format for easier portability across devices.\n",
    "\n",
    "Converting to ONNX is a simple command, where we've correctly references our model's weights file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1d533c-a1ae-4a9d-97fb-6b16cb903548",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install onnx>=1.10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981c26b5-780a-4ab3-95a4-886460a00879",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ./deploy/ONNX/export_onnx.py \\\n",
    "    --weights ./runs/train/exp/weights/best_ckpt.pt \\\n",
    "    --img-size 512 512 \\\n",
    "    --simplify \\\n",
    "    --iou-thres 0.35 \\\n",
    "    --conf-thres 0.35 \\\n",
    "    --dynamic-batch \\\n",
    "    --ort \\"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
